{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUfbCnAYSKQS6rpa+o7eox",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arasmasood/aras/blob/master/Copy_of_Untitled19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwvcqChSFaPt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import struct\n",
        "import socket\n",
        "import time\n",
        "import datetime\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "import tkinter as tk  # importing Tkinter library\n",
        "from collections.abc import Iterable\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "# import matplotlib.pylab as plt\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from time import perf_counter, sleep\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tkinter import *\n",
        "from tkinter.ttk import *\n",
        "from sklearn import preprocessing\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "import lightgbm as lgb\n",
        "global df_rows\n",
        "global df\n",
        "global file_path\n",
        "global data\n",
        "global label_Anomaly_data\n",
        "global label_Normal_data\n",
        "from BorutaShap import BorutaShap, load_data\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "import shap\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# copy\n",
        "class MyApp:\n",
        "    def __init__(self):\n",
        "        \n",
        "       \n",
        "        self.df = pd.read_csv(\"C:\\\\Users\\\\micro\\\\IOTDATA\\\\IOTTT.csv\")\n",
        "        self.pre_Process(self.df)\n",
        "        cleaned_data = self.feature_scaling(self.df)\n",
        "        \n",
        "        #print(\"The execution time for preprocssing and feature scaling is: \")\n",
        "        #self.elapced_time(self.start,time.time())\n",
        "        \n",
        "        #self.feature_selection_RandomForest(cleaned_data)\n",
        "        self.feature_selection(cleaned_data)\n",
        "        #self.feature_selection_XGBoost(cleaned_data)\n",
        "        #self.feature_selection_GBoost(cleaned_data)\n",
        "    def feature_selection_GBoost(self,df):\n",
        "        print(\"\\n**** Feature Selection Starting GBoost****\\n\")\n",
        "        \n",
        "        X = df.drop(\"Label\",axis=1)\n",
        "        y = df[\"Label\"]\n",
        "        \n",
        "        numNormal = len(self.df[self.df[\"Label\"]==1.0])\n",
        "        numAnomaly = len(self.df[self.df[\"Label\"]==0.0])\n",
        "        \n",
        "        SPW = numAnomaly / numNormal\n",
        "        \n",
        "        model = GradientBoostingClassifier(n_estimators=10)\n",
        "        Feature_Selector = BorutaShap(model=model,\n",
        "                              importance_measure='shap',\n",
        "                              classification=True)\n",
        "        start = time.time()\n",
        "        Feature_Selector.fit(X=X, y=y, n_trials=10, sample=False,\n",
        "                              normalize=True,\n",
        "                     verbose=True)\n",
        "        end = time.time()\n",
        "        Feature_Selector.plot(which_features='all')\n",
        "        \n",
        "        subset = Feature_Selector.Subset()\n",
        "\n",
        "        print(subset)\n",
        "        print(\"\\n ***** Feature Selection End *****\\n\")\n",
        "        \n",
        "        print(\"The execution time for GBoost Algorithm is: \")\n",
        "        print(end-start)\n",
        "        \n",
        "    def feature_selection_XGBoost(self, df):\n",
        "        #---------------splitting-----------------------------\n",
        "        print(\"\\n ***** Feature Selection Startin XGB *****\\n\")\n",
        "        X = df.drop('Label', axis=1)\n",
        "        y = df['Label']\n",
        "        numNormal = len(self.df[self.df['Label'] == 1.0])\n",
        "        numAnomaly = len(self.df[self.df['Label'] == 0.0])\n",
        "\n",
        "       \n",
        "  \n",
        "        SPW = numAnomaly / numNormal\n",
        "        \n",
        "        model = XGBClassifier( scale_pos_weight=SPW, objective='binary:logistic', random_state=1,tree_method='gpu_hist', gpu_id=0)\n",
        "        Feature_Selector = BorutaShap(model=model,\n",
        "                              importance_measure='shap',\n",
        "                              classification=True)\n",
        "        start = time.time()\n",
        "        Feature_Selector.fit(X=X, y=y, n_trials=50, sample=False,\n",
        "                              normalize=True,\n",
        "                     verbose=True)\n",
        "        end = time.time()\n",
        "        Feature_Selector.plot(which_features='all')\n",
        "        \n",
        "        subset = Feature_Selector.Subset()\n",
        "\n",
        "        print(subset)\n",
        "        print(\"\\n ***** Feature Selection End *****\\n\")\n",
        "        \n",
        "        print(\"The execution time for XGBoost Algorithm is: \")\n",
        "        print(end-start)\n",
        "        \n",
        "    def feature_selection_RandomForest(self, df):\n",
        "        #---------------splitting-----------------------------\n",
        "        print(\"\\n ***** Feature Selection Startin RandomForest *****\\n\")\n",
        "        X = df.drop('Label', axis=1)\n",
        "        y = df['Label']\n",
        "    \n",
        "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, stratify=y, shuffle=True)\n",
        "        \n",
        "        model = BalancedRandomForestClassifier()\n",
        "        Feature_Selector = BorutaShap(model=model,\n",
        "                        importance_measure='shap',\n",
        "                              classification=True)\n",
        "        start = time.time()\n",
        "        Feature_Selector.fit(X_train, y_train,n_trials=50)\n",
        "        end = time.time()\n",
        "        \n",
        "        #sel.get_support()\n",
        "        #selected_feat = X_train.columns[(sel.get_support())]\n",
        "        #len(selected_feat)\n",
        "        selected_columns = list()\n",
        "        selected_columns.append(sorted(Feature_Selector.Subset().columns))\n",
        "        print(len(selected_columns))\n",
        "        print(\"Important column names: \")\n",
        "        print(selected_columns)\n",
        "        \n",
        "        print(\"\\n ***** Feature Selection End *****\\n\")\n",
        "        \n",
        "        print(\"The execution time for RandomForest Algorithm is: \")\n",
        "        print(end-start)\n",
        "    \n",
        "    def feature_selection(self, df):\n",
        "        #---------------splitting-----------------------------\n",
        "        X = df.drop('Label', axis=1)\n",
        "        y = df['Label']\n",
        "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, stratify=y, shuffle=True)       \n",
        "        #-------------------------------------------       \n",
        "        print(\"\\n ***** Feature Selection Starts *****\\n\")\n",
        "        \n",
        "        \n",
        "        print(\"\\n X Section \\n\")\n",
        "        print(X)\n",
        "        \n",
        "        print(\"\\n Y Section \\n\")\n",
        "        print(y)\n",
        "        \n",
        "        model = RandomForestClassifier()\n",
        "        Feature_Selector = BorutaShap(model=model,importance_measure='shap',\n",
        "                                  classification=True)\n",
        "        start = time.time()\n",
        "        Feature_Selector.fit(X=X_train, y=y_train, n_trials=50, sample=False,\n",
        "                            normalize=True,\n",
        "                             verbose=True)\n",
        "        end = time.time()\n",
        "        Feature_Selector.plot(which_features='all')\n",
        "        subset = Feature_Selector.Subset()\n",
        "\n",
        "        print(subset)\n",
        "        \n",
        "        print(\"\\n ***** Feature Selection End *****\\n\")\n",
        "        \n",
        "        print(\"The execution time for feauter selection is: \")\n",
        "        print(end-start)\n",
        "\n",
        "\n",
        "    def normal_And_Anom_lable(self, excel_filename):\n",
        "        global dataframe\n",
        "        dataframe = pd.read_csv(excel_filename)\n",
        "        Number_of_Rows = len(self.df)\n",
        "\n",
        "        Normal = dataframe[\"Label\"].sum().count('Normal')\n",
        "        Anomaly = dataframe[\"Label\"].sum().count('Anomaly')\n",
        "\n",
        "        self.df.Label.apply(lambda x: 0 if x == 'Normal' else 1)\n",
        "        Lable_for_Normal_Data = 'Number of Normals     : ' + str(Normal) + \" {Normals are  = 0 }\"\n",
        "        Lable_for_Anomal_Data = 'Number of Anomalys    : ' + str(Anomaly) + \" {Anomalys are = 1 }\"\n",
        "\n",
        "        LND = Lable_for_Normal_Data\n",
        "        Anomal_Datas = Lable_for_Anomal_Data\n",
        "        label_Anomaly_data[\"text\"] = Anomal_Datas\n",
        "        label_Normal_data[\"text\"] = LND\n",
        "\n",
        "\n",
        "    def pre_Process(self, df):\n",
        "        print(\"\\n ***** Pre Processing Starts ***** \\n\")\n",
        "\n",
        "        str = self.isInf_isNan(self.df)\n",
        "\n",
        "        str1 = self.Duplicaete(self.df)\n",
        "\n",
        "        str2 = self.drop_Dataframe(self.df)\n",
        "\n",
        "        str3 = self.Convert_Ip(self.df)\n",
        "\n",
        "        str4 = self.label_Creator(self.df)\n",
        "\n",
        "        strTotal = str + str1 + str2 + str3 + str4\n",
        "\n",
        "        X = self.df.iloc[:, 0:-1]\n",
        "        y = self.df.iloc[:, -1]\n",
        "\n",
        "        label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "        # Encode labels in column 'species'.\n",
        "        self.df['Label'] = label_encoder.fit_transform(self.df['Label'])\n",
        "\n",
        "        self.df['Label'].unique()\n",
        "\n",
        "        print(\"\\n\", strTotal, \"\\n\")\n",
        "\n",
        "        print(self.df)\n",
        "\n",
        "        print(\"\\n ***** Pre Processing Ends ***** \\n\")\n",
        "\n",
        "\n",
        "    def feature_scaling(self, df):\n",
        "        print(\"\\n ***** Feature Scaling Starts ***** \\n\")\n",
        "        # label_tmp = self.df[\"Label\"]\n",
        "        X = self.df.iloc[:, 0:-1]\n",
        "        y = self.df.iloc[:, -1]\n",
        "\n",
        "\n",
        "        columnsNames = list(self.df.columns)\n",
        "\n",
        "        # feature scaling code\n",
        "        scaler = MinMaxScaler()\n",
        "        X = scaler.fit_transform(X)\n",
        "\n",
        "        df_X = pd.DataFrame(X, columns=columnsNames[:-1])\n",
        "        df_X.reset_index(drop=True, inplace=True)\n",
        "        df_Y = pd.DataFrame(y)\n",
        "        df_Y.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        self.df = pd.concat([df_X, df_Y] ,axis=1)\n",
        "\n",
        "\n",
        "\n",
        "        print(self.df)\n",
        "        #self.df=self.df[0:1000]\n",
        "        #self.df.to_csv(\"C:\\\\Users\\\\micro\\\\IOTDATA\\\\Output.csv\")\n",
        "    \n",
        "        return self.df\n",
        "        print(\"\\n ***** Feature Scaling Ends ***** \\n\")\n",
        "\n",
        "\n",
        "    def isInf_isNan(self, df):\n",
        "        # Removing NULL and INF Code Starts\n",
        "        is_nan = self.df.isin([np.nan]).any(axis=1)\n",
        "        is_inf = self.df.isin([np.inf, -np.inf]).any(axis=1)\n",
        "        tmp1 = str(self.df[is_inf].shape[0])\n",
        "        tmp2 = str(self.df[is_nan].shape[0])\n",
        "        prev_count = self.df.shape[0]\n",
        "        self.df = self.df[~df.isin([np.nan, np.inf, -np.inf]).any(axis=1)]\n",
        "        aft_count = self.df.shape[0]\n",
        "        isNanisInfResult = \"INF Found: \" + tmp1 + \" || NaN Found: \" + tmp2 + \"\\n\"\n",
        "        isNanisInfResult += \"Removed {} Now {}\".format(prev_count - aft_count, self.df.shape) + \" \"\n",
        "\n",
        "        return isNanisInfResult\n",
        "\n",
        "    def Duplicaete(self, df):\n",
        "        # Removing Duplicates Code Starts\n",
        "        sum = self.df.duplicated().sum()\n",
        "        # df.drop_duplicates(keep='first', inplace=True, ignore_index=True)\n",
        "        self.df.drop_duplicates(subset=None, keep='first', inplace=True, ignore_index=True)\n",
        "\n",
        "        dupResult = \"\\n-------------------------------\\n\"\n",
        "        dupResult += str(sum) + \" are duplicates and removed\"\n",
        "        return dupResult\n",
        "\n",
        "    # Removing Duplicates Code Ends\n",
        "    def drop_Dataframe(self, df):\n",
        "        self.df.drop([\"Flow_ID\", \"Timestamp\", \"Cat\", \"Sub_Cat\"], axis=1, inplace=True)\n",
        "        dropDFResult = \"\\n-------------------------------\\n\"\n",
        "        dropDFResult += \"Flow_ID, Timestamp, Cat, Sub_Cat Columns are removed\"\n",
        "\n",
        "        return dropDFResult\n",
        "\n",
        "    def ip2int(self, addr):\n",
        "        # Changing SRC_IP, SDT_IP To Numeric Code Starts\n",
        "\n",
        "        # changes the ip addresses to integer\n",
        "        # print(\"* Changing SRC_IP, SDT_IP To Numeric *\")\n",
        "        return struct.unpack(\"!I\", socket.inet_aton(addr))[0]\n",
        "\n",
        "    def Convert_Ip(self, df):\n",
        "        # Changing SRC_IP, SDT_IP To Numeric Code Ends\n",
        "        # saving the resut inside the columns\n",
        "        self.df['Src_IP'] = df.Src_IP.apply(lambda x: self.ip2int(x))\n",
        "        self.df['Dst_IP'] = df.Dst_IP.apply(lambda x: self.ip2int(x))\n",
        "\n",
        "        convertIPResult = \"\\n-------------------------------\\n\"\n",
        "        convertIPResult += \"Src_IP and  Dst_IP are converted to Int64 from Object\"\n",
        "        return convertIPResult\n",
        "\n",
        "    def label_Creator(self, df):\n",
        "        Type_of_data = self.df['Label']\n",
        "        Type_of_data.replace({\"Normal\": \"0\", \"Anomaly\": \"1\", 'NUll': \"1\"}, inplace=True)\n",
        "\n",
        "        Normal = self.df[\"Label\"].sum().count('0')\n",
        "        Anomaly = self.df[\"Label\"].sum().count('1')\n",
        "        labelCreator = \"\\n-------------------------------\\n\"\n",
        "        labelCreator += \"Normal -> \" + str(Normal) + \" || Anomaly ->\" + str(Anomaly)\n",
        "\n",
        "        return labelCreator\n",
        "\n",
        "    # Models\n",
        "    def XGBoostClassiferWithCV(self, df):\n",
        "\n",
        "        print(\"\\n *******************************\")\n",
        "        print(\"******XGBoost with CV*******\")\n",
        "        print(\"*******************************\")\n",
        "        numNormal = len(self.df[self.df['Label'] == 1.0])\n",
        "        numAnomaly = len(self.df[self.df['Label'] == 0.0])\n",
        "\n",
        "        X = self.df.iloc[:, 0:-1]\n",
        "        y = self.df.iloc[:, -1]\n",
        "        print(\"The X values: \")\n",
        "        print(X)\n",
        "\n",
        "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, stratify=y, shuffle=True)\n",
        "\n",
        "        SPW = numAnomaly / numNormal\n",
        "        xgb = XGBClassifier(scale_pos_weight=SPW, objective='binary:logistic', random_state=1, tree_method='gpu_hist', gpu_id=0)\n",
        "        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
        "\n",
        "\n",
        "\n",
        "        cv_start = time.time()\n",
        "        n_scores = cross_val_score(xgb, X, y, scoring='accuracy', cv=skf, n_jobs=-1, error_score='raise')\n",
        "        cv_end = time.time()\n",
        "\n",
        "        print('Cross validation score accuracy average for the model: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
        "        print(\"Accuracy values: \", np.round(n_scores, 4))\n",
        "        print(\"\\nTime consumed for Cross validation: %4.3f\" % (cv_end - cv_start))\n",
        "\n",
        "    def XGBoostClassiferWithOutCV(self, df):\n",
        "        print(\"\\n *******************************\")\n",
        "        print(\"******XGBoost without CV*******\")\n",
        "        print(\"*******************************\")\n",
        "        numNormal = len(self.df[self.df['Label'] == 1.0])\n",
        "        numAnomaly = len(self.df[self.df['Label'] == 0.0])\n",
        "\n",
        "        X = self.df.iloc[:, 0:-1]\n",
        "        y = self.df.iloc[:, -1]\n",
        "\n",
        "        print(X)\n",
        "\n",
        "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, stratify=y, shuffle=True)\n",
        "\n",
        "        SPW = numAnomaly / numNormal\n",
        "\n",
        "        xgb = XGBClassifier(scale_pos_weight=SPW, objective='binary:logistic', random_state=1,tree_method='gpu_hist', gpu_id=0)\n",
        "\n",
        "        training_start = time.time()\n",
        "\n",
        "        xgb.fit(X_train, y_train)\n",
        "\n",
        "        training_end = time.time()\n",
        "\n",
        "        prediction_start = time.time()\n",
        "\n",
        "        preds = xgb.predict(X_valid)\n",
        "\n",
        "        prediction_end = time.time()\n",
        "\n",
        "        preds_train = xgb.predict(X_train)\n",
        "\n",
        "        accuracy_test = accuracy_score(y_valid, preds)  # compute test accuracy\n",
        "        accuracy_tarin = accuracy_score(y_train, preds_train)  # compute train accuracy\n",
        "\n",
        "        cm = confusion_matrix(y_valid, preds)\n",
        "\n",
        "        print(\"XGBoost's testing accuracy is: %3.4f\" % (accuracy_test * 100))\n",
        "        print(\"XGBoost's training accuracy is: %3.4f\" % (accuracy_tarin * 100))\n",
        "\n",
        "        print(\"Precision:{}\", precision_score(y_valid, preds))\n",
        "        print(\"Recall:{}\", recall_score(y_valid, preds))\n",
        "\n",
        "        print(\"\\nTime consumed for training the model %4.3f\" % (training_end - training_start))\n",
        "        print(\"Time consumed for testing: %6.5f seconds\" % (prediction_end - prediction_start))\n",
        "\n",
        "        print(\"Confusion Matrix: \\n\", cm)\n",
        "\n",
        "\n",
        "    def LightGBMClassifer(self, df):\n",
        "        print(\"\\n *******************************\")\n",
        "        print(\"******LGBM with CV*******\")\n",
        "        print(\"*******************************\")\n",
        "        numNormal = len(self.df[self.df['Label'] == 1.0])\n",
        "        numAnomaly = len(self.df[self.df['Label'] == 0.0])\n",
        "\n",
        "        X = self.df.iloc[:, 0:-1]\n",
        "        y = self.df.iloc[:, -1]\n",
        "\n",
        "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, stratify=y, shuffle=True)\n",
        "\n",
        "        SPW = numAnomaly / numNormal\n",
        "\n",
        "        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
        "        lgbm = LGBMClassifier(scale_pos_weight=SPW, random_state=1)\n",
        "\n",
        "        cv_start = time.time()\n",
        "        n_scores = cross_val_score(lgbm, X, y, scoring='accuracy', cv=skf, n_jobs=-1, error_score='raise')\n",
        "        cv_end = time.time()\n",
        "\n",
        "        print('Cross validation score accuracy average for the model: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
        "        print(\"Accuracy values: \", np.round(n_scores, 4))\n",
        "        print(\"\\nTime consumed for Cross validation: %4.3f\" % (cv_end - cv_start))\n",
        "\n",
        "    def LightGBMClassiferWithOutCV(self, df):\n",
        "        print(\"\\n *******************************\")\n",
        "        print(\"******LGBM without CV*******\")\n",
        "        print(\"*******************************\")\n",
        "        numNormal = len(self.df[self.df['Label'] == 1.0])\n",
        "        numAnomaly = len(self.df[self.df['Label'] == 0.0])\n",
        "\n",
        "        X = self.df.iloc[:, 0:-1]\n",
        "        y = self.df.iloc[:, -1]\n",
        "\n",
        "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, stratify=y, shuffle=True)\n",
        "\n",
        "        SPW = numAnomaly / numNormal\n",
        "\n",
        "        lgbm = LGBMClassifier(scale_pos_weight=SPW, random_state=1)\n",
        "\n",
        "        training_start = time.time()\n",
        "\n",
        "        lgbm.fit(X_train, y_train)  # training phase\n",
        "\n",
        "        training_end = time.time()\n",
        "\n",
        "        prediction_start = time.time()\n",
        "\n",
        "        preds = lgbm.predict(X_valid)  # test phase\n",
        "\n",
        "        prediction_end = time.time()\n",
        "\n",
        "        # prediction_train_start = time.time()\n",
        "\n",
        "        preds_train = lgbm.predict(X_train)\n",
        "\n",
        "        # prediction_train_end = time.time()\n",
        "\n",
        "        accuracy_test = accuracy_score(y_valid, preds)\n",
        "        accuracy_tarin = accuracy_score(y_train, preds_train)\n",
        "\n",
        "        cm = confusion_matrix(y_valid, preds)\n",
        "\n",
        "        print(\"LightBoost's testing accuracy is: %3.4f\" % (accuracy_test * 100))\n",
        "        print(\"LightBoost's training accuracy is: %3.4f\" % (accuracy_tarin * 100))\n",
        "\n",
        "        print(\"Precision:{}\", precision_score(y_valid, preds))\n",
        "        \n",
        "        print(\"Recall:{}\", recall_score(y_valid, preds))\n",
        "\n",
        "        print(\"\\nTime consumed for training the model %4.3f\" % (training_end - training_start))\n",
        "        print(\"Time consumed for testing: %6.5f seconds\" % (prediction_end - prediction_start))\n",
        "\n",
        "\n",
        "        print(\"Confusion Matrix: \\n\", cm)"
      ],
      "metadata": {
        "id": "M2veCx1sLqFQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}